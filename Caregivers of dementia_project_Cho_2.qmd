---
title: "Predicting Mental Health Problems from Social Determinants of Health and Caregiving Activities of Caregivers of Persons with Dementia: A Machine Learning Approach"
subtitle: "BMIN503/EPID600 Final Project"
author: "Hannah Cho"
format: html
editor: visual
number-sections: true
embed-resources: true
---

------------------------------------------------------------------------

# Overview {#sec-overview}

In the United States, as of 2021, approximately 34 million informal caregivers were providing unpaid support to persons with dementia (PWD). As dementia progresses to its end-stage or terminal phase, individuals with the condition often become fully dependent on caregivers for assistance with most daily activities, such as bathing, repositioning, and other essential personal care needs. Notably, 80% of PWD receive care from informal caregivers—primarily spouses, adult children, and close friends—within community settings rather than institutionalized environments. The project addressed public health issues related to dementia caregiving, which directly affect caregivers' emotional problems, such as depression and anxiety, by predicting key caregiving activities and sociodemographic features of those at high risk.

This project was conducted after consultations with Dr. George Demiris, a recognized expert in the field of dementia care and caregiving, and Dr. Huang, an experienced biostatistician. Their contributions provided critical insights into both the substantive and methodological aspects of the research, ensuring a rigorous and well-informed approach to addressing the challenges faced by caregivers of PLWD.

# Introduction {#sec-introduction}

In the United States, as of 2021, 34 million informal caregivers were providing unpaid support to persons living with dementia (PWD; Alzheimer’s Association, 2023; McCabe et al., 2016; Reinhard et al., 2023). As PWD progress to the end-stage or terminal stage of dementia, they often become dependent on assistance for most daily activities, including bathing and changing position. Of all PWD, 80% receive care from individuals—often spouses, adult children, and close friends—in the community (Alzheimer’s Association, 2023).

Dementia caregivers face profoundly complex and multifaceted challenges that extend beyond the physical and emotional demands of caregiving. These challenges encompass physical tasks, such as providing continuous care, assisting with activities of daily living, and managing the multifarious health complications associated with dementia. However, the burden of caregiving is not solely physical; it also places a considerable emotional burden on caregivers. Many experience elevated levels of stress, anxiety, depression, and isolation, as the exhaustive nature of caregiving leaves little opportunity for self-care or social engagement. Some studies reported caregivers of PWD (Sorensen & Conwell, 2011) are particularly vulnerable to anxiety and depression due to the progressive and unpredictable trajectory of the disease . This trajectory often varies significantly based on individuals' pre-existing conditions and comorbidities, adding to caregivers' uncertainty and emotional strain.

**Research Question**: This research seeks to identify key predictors of specific caregiving features and sociodemographic factors among dementia caregivers that are related to mental health outcomes. Additionally, it evaluates the predictive accuracy of machine learning algorithms in supporting the early identification of caregivers at risk of mental health issues.

**Framework**:

The World Health Organization’s conceptual framework for social determinants of health identifies factors such as gender, race/ethnicity, education, occupation, and economic status as influential. Living conditions and social support also play a role in an individual's health and well-being. Based on this framework and available dataset information, a conceptual framework was modified for this study.

# Methods {#sec-methods}

## **Dataset**:

This study used data from the National Health and Aging Trends Study (NHATS) Round 11 and the National Study of Caregiving (NSOC) Round 4, which include data collected in 2021. The NHATS is a publicly accessible dataset that includes a nationally representative sample of adults aged 65 years and older who are Medicare beneficiaries in the United States of America. The NHATS began in 2011 and included 8,245 participants who have been followed up annually since then; the study goal is to encourage research to maximize health and enhance the quality of life of older adults. The NSOC is conducted alongside the NHATS; participants in the NSOC are caregivers for older adults included in the NHATS. Both the NHATS and the NSOC were funded by the National Institute on Aging (R01AG062477; U01AG032947). When used together, the NHATS and NSOC provide valuable information on dyads of older adults receiving care and their family caregivers.

## **Samples**:

PWD with dementia: Probable dementia was identified based on one of the following criteria: a self-reported diagnosis of dementia or Alzheimer’s disease by a physician, a score of 2 or higher on the AD8 screening instrument administered to proxy respondents, or a score that is 1.5 standard deviations below the mean on a range of cognitive tests.Caregivers: Caregivers are identified from the NSOC and NHATS dataset. As the purpose of this study was to examine family caregivers who provide care to persons living with dementia at home, we selected those who provided care to persons living with dementia and lived at home (r11dresid) using the Dementia Classification (demclass) with Programming Statements provided by the NHATS.

### Loading Packages and Bringing Dataset

```{r}
#Loading necessary packages first.
library(haven) #dta file
library(dplyr) #data cleaning
library(ggplot2) #data visualization 
library(gtsummary) #summary statistics
library(haven) #dta file
library(dplyr) #data cleaning
library(ggplot2) #data visualization 
library(gtsummary) #summary statistics
library(Hmisc) #Pearson correlation coefficient
library(tibble) 
library(kernlab)
library(randomForest)
library(glmnet)
library(xgboost)
library(vip)
library(tidymodels)
library(yardstick)
library(pROC)
library(lattice)
library(caret)
library(broom)
library(tune)
library(parsnip)
library(recipes)
library(rsample)
library(workflows)
tidymodels_prefer()
```

```{r}
#Bring datasets 
df1 <- read_dta("~/R  HC/BMIN503_Final_Project/final final/NHATS_Round_11_SP_File_V2.dta") # dementia classfication in this file
df2 <- read_dta("~/R  HC/BMIN503_Final_Project/final final/NSOC_r11.dta") #caregiver information 1
df3 <- read_dta("~/R  HC/BMIN503_Final_Project/final final/NSOC_cross.dta") #caregiver information 2
df4 <- read_dta("~/R  HC/BMIN503_Final_Project/final final/NHATS_Round_11_OP_File.dta") #older adults information 
```

### Choosing Probable and Possible Dementia Participants Given By NHATS

```{r}
#need to clean df1 first in order to classify dementia classes  
#ENTER WHICH ROUND?
sp1 <- df1 |>
  mutate(rnd = 11) 

#3. EDIT ROUND NUMBER INSIDE THE QUOTES 
#(THIS REMOVES THE PREFIXES ON NEEDED VARIABLES ) 
sp1 <- sp1 |>
  rename_all(~stringr::str_replace(.,"^r11","")) |>
  rename_all(~stringr::str_replace(.,"^hc11","")) |>
  rename_all(~stringr::str_replace(.,"^is11","")) |>
  rename_all(~stringr::str_replace(.,"^cp11","")) |> 
  rename_all(~stringr::str_replace(.,"^cg11",""))

#ADD R1DAD8DEM AND SET TO -1 FOR ROUND 1 BECAUSE THERE IS NO PRIOR DIAGNOSIS IN R1
sp1 <- sp1 |>
  mutate(dad8dem = ifelse(rnd == 1, -1, dad8dem))


#ADD R1DAD8DEM AND SET TO -1 FOR ROUND 1 BECAUSE THERE IS NO PRIOR DIAGNOSIS IN R1
sp1 <- sp1 |> 
  mutate(dad8dem = ifelse(rnd == 1, -1, dad8dem))

#SUBSET NEEDED VARIABLES
df<-sp1 |> 
  dplyr::select(spid, rnd, dresid, resptype, disescn9, chgthink1, chgthink2, chgthink3, chgthink4, chgthink5, chgthink6, chgthink7, chgthink8, dad8dem,
                speaktosp, todaydat1, todaydat2, todaydat3, todaydat4, todaydat5, presidna1, presidna3, vpname1, vpname3, quesremem, dclkdraw, atdrwclck, 
                dwrdimmrc, dwrdlstnm, dwrddlyrc)

#FIX A ROUND 2 CODING ERROR#
df <- df |>
  mutate(dwrdimmrc = ifelse(dwrdimmrc==10 & dwrddlyrc==-3 & rnd==2, -3, dwrdimmrc))

#CREATE SELECTED ROUND DEMENTIA CLASSIFICATION VARIABLE 
df <- df |>
  mutate(demclas  =  ifelse(dresid==3 | dresid==5 | dresid==7, -9, #SET MISSING (RESIDENTIAL CARE FQ ONLY) AND N.A. (NURSING HOME RESIDENTS, DECEASED)
                            ifelse((dresid==4 & rnd==1) | dresid==6 | dresid==8, -1,                #SET MISSING (RESIDENTIAL CARE FQ ONLY) AND N.A. (NURSING HOME RESIDENTS, DECEASED)
                                   ifelse((disescn9==1 | disescn9==7) &           #CODE PROBABLE IF DEMENTIA DIAGNOSIS REPORTED BY SELF OR PROXY*
                                            (resptype==1 | resptype==2), 1, NA))))

#CODE AD8_SCORE*
#INITIALIZE COUNTS TO NOT APPLICABLE*
#ASSIGN VALUES TO AD8 ITEMS IF PROXY AND DEMENTIA CLASS NOT ALREADY ASSIGNED BY REPORTED DIAGNOSIS 
for(i in 1:8){
  df[[paste("ad8_", i, sep = "")]]  <- as.numeric(ifelse(df[[paste("chgthink", i, sep = "")]]==2 & df$resptype==2 & is.na(df$demclas), 0, #PROXY REPORTS NO CHANGE
                                                         ifelse((df[[paste("chgthink", i, sep = "")]]==1 | df[[paste("chgthink", i, sep = "")]] == 3) & df$resptype==2 & is.na(df$demclas), 1, #PROXY REPORTS A CHANGE OR ALZ/DEMENTIA*
                                                                ifelse(df$resptype==2 & is.na(df$demclas), NA, -1))))    #SET TO NA IF IN RES CARE AND demclass=., OTHERWISE AD8 ITEM IS SET TO NOT APPLICABLE                                                                                                                        
}

#INITIALIZE COUNTS TO NOT APPLICABLE*
for(i in 1:8){
  df[[paste("ad8miss_", i, sep = "")]]  <- as.numeric(ifelse(is.na(df[[paste("ad8_", i, sep = "")]]), 1,
                                                             ifelse((df[[paste("ad8_", i, sep = "")]]==0 | df[[paste("ad8_", i, sep = "")]]==1) & df$resptype==2 & is.na(df$demclas), 0, -1)))
}

for(i in 1:8){
  df[[paste("ad8_", i, sep = "")]] <- as.numeric(ifelse(is.na(df[[paste("ad8_", i, sep = "")]]) & is.na(df$demclas) & df$resptype==2, 0, df[[paste("ad8_", i, sep = "")]]))
}

#COUNT AD8 ITEMS
#ROUNDS 2+
df <- df |>
  mutate(ad8_score = ifelse(resptype==2 & is.na(demclas), (ad8_1 + ad8_2 + ad8_3 + ad8_4 + ad8_5 + ad8_6 + ad8_7 + ad8_8), -1)) %>% 
  #SET PREVIOUS ROUND DEMENTIA DIAGNOSIS BASED ON AD8 TO AD8_SCORE=8 
  mutate(ad8_score = ifelse(dad8dem==1 & resptype==2 & is.na(demclas), 8, ad8_score))  %>% 
  #SET PREVIOUS ROUND DEMENTIA DIAGNOSIS BASED ON AD8 TO AD8_SCORE=8 FOR ROUNDS 4-9
  mutate(ad8_score = ifelse(resptype==2 & dad8dem==-1 & chgthink1==-1 & (rnd>=4 & rnd<=9) & is.na(demclas) , 8, ad8_score)) 

#COUNT MISSING AD8 ITEMS
df <- df |> 
  mutate(ad8_miss = ifelse(resptype==2 & is.na(demclas),(ad8miss_1+ad8miss_2+ad8miss_3+ad8miss_4+ad8miss_5+ad8miss_6+ad8miss_7+ad8miss_8), -1))

#CODE AD8 DEMENTIA CLASS 
#IF SCORE>=2 THEN MEETS AD8 CRITERIA
#IF SCORE IS 0 OR 1 THEN DOES NOT MEET AD8 CRITERIA
df <- df |> 
  mutate(ad8_dem = ifelse(ad8_score>=2, 1,
                          ifelse(ad8_score==0 | ad8_score==1 | ad8_miss==8, 2, NA)))

#UPDATE DEMENTIA CLASSIFICATION VARIABLE WITH AD8 CLASS
df <- df |> 
  #PROBABLE DEMENTIA BASED ON AD8 SCORE  
  mutate(demclas = ifelse(ad8_dem==1 & is.na(demclas), 1, 
                          #NO DIAGNOSIS, DOES NOT MEET AD8 CRITERION, AND PROXY SAYS CANNOT ASK SP COGNITIVE ITEMS*
                          ifelse(ad8_dem==2 & speaktosp==2 & is.na(demclas), 3, demclas)))


####CODE DATE ITEMS AND COUNT 
#CODE ONLY YES/NO RESPONSES: MISSING/NA CODES -1, -9 LEFT MISSING*
#2: NO/DK OR -7: REFUSED RECODED TO : NO/DK/RF*
#****ADD NOTES HERE ABOUT WHAT IS HAPPENING IN ROUNDS 1-3, 5+ VS. ROUND 4 
#*
for(i in 1:5){
  df[[paste("date_item", i, sep = "")]]  <- as.numeric(ifelse(df[[paste("todaydat", i, sep = "")]]==1, 1,
                                                              ifelse(df[[paste("todaydat", i, sep = "")]]==2 | df[[paste("todaydat", i, sep = "")]]== -7, 0, NA)))
}

#COUNT CORRECT DATE ITEMS
df <- df |>
  mutate(date_item4 = ifelse(rnd==4, date_item5, date_item4)) %>% 
  mutate(date_sum = date_item1 + date_item2 + date_item3 + date_item4) %>% 
  
  #PROXY SAYS CAN'T SPEAK TO SP
  mutate(date_sum = ifelse(speaktosp==2 & is.na(date_sum),-2,  
                           #PROXY SAYS CAN SPEAK TO SP BUT SP UNABLE TO ANSWER*
                           ifelse((is.na(date_item1) | is.na(date_item2) | is.na(date_item3) | is.na(date_item4)) & speaktosp==1,-3, date_sum))) %>% 
  
  #MISSING IF PROXY SAYS CAN'T SPEAK TO SP*  
  mutate(date_sumr = ifelse(date_sum == -2 , NA, 
                            #0 IF SP UNABLE TO ANSWER*
                            ifelse(date_sum == -3 , 0, date_sum)))


########PRESIDENT AND VICE PRESIDENT NAME ITEMS AND COUNT########## 
##CODE ONLY YES/NO RESPONSES: MISSING/N.A. CODES -1,-9 LEFT MISSING *
##2:NO/DK OR -7:REFUSED RECODED TO 0:NO/DK/RF*
df <- df |>
  mutate(preslast = ifelse(presidna1 == 1, 1,
                           ifelse(presidna1 == 2 | presidna1 == -7, 0, NA))) |> 
  mutate(presfirst = ifelse(presidna3 == 1, 1,
                            ifelse(presidna3 == 2 | presidna3 == -7, 0, NA))) |> 
  mutate(vplast = ifelse(vpname1 == 1, 1,
                         ifelse(vpname1 == 2 | vpname1 == -7, 0, NA))) |> 
  mutate(vpfirst = ifelse(vpname3 == 1, 1,
                          ifelse(vpname3 == 2 | vpname3 == -7, 0, NA))) |> 
  
  #COUNT CORRECT PRESIDENT/VP NAME ITEMS*
  mutate(presvp = preslast + presfirst + vplast + vpfirst) |> 
  #PROXY SAYS CAN'T SPEAK TO SP 
  mutate(presvp = ifelse(speaktosp == 2 & is.na(presvp), -2, 
                         #PROXY SAYS CAN SPEAK TO SP BUT SP UNABLE TO ANSWER                           
                         ifelse((is.na(preslast) | is.na(presfirst) | is.na(vplast) | is.na(vpfirst)) & speaktosp==1 & is.na(presvp),-3, presvp))) |> 
  
  #MISSING IF PROXY SAYS CAN’T SPEAK TO SP*
  mutate(presvpr =  ifelse(presvp == -2 , NA, 
                           ifelse(presvp == -3 , 0, presvp))) |> 
  
  #ORIENTATION DOMAIN: SUM OF DATE RECALL AND PRESIDENT/VP NAMING* 
  mutate(date_prvp = date_sumr + presvpr)


#######EXECUTIVE FUNCTION DOMAIN: CLOCK DRAWING SCORE##########
#RECODE DCLKDRAW TO ALIGN WITH MISSING VALUES IN PREVIOUS ROUNDS (ROUND 10 ONLY)* 
df <- df |>
  mutate(dclkdraw = ifelse(speaktosp == 2 & dclkdraw == -9 & rnd==10, -2,
                           ifelse(speaktosp==1 & (quesremem==2 | quesremem==-7 | quesremem==-8) & dclkdraw==-9 & rnd==10, -3,
                                  ifelse(atdrwclck==2 & dclkdraw==-9 & rnd==10, -4,
                                         ifelse(atdrwclck==97 & dclkdraw==-9 & rnd==10, -7, dclkdraw)))))

#RECODE DCLKDRAW TO ALIGN WITH MISSING VALUES IN PREVIOUS ROUNDS (ROUNDS 11 AND FORWARD ONLY)* 
df<-df  |>
  mutate(dclkdraw = ifelse(speaktosp == 2 & dclkdraw == -9 & rnd>=11, -2, 
                           ifelse(speaktosp == 1 & (quesremem == 2 | quesremem == -7 | quesremem == -8) & dclkdraw == -9, -3 & rnd>=11, dclkdraw))) 
df<-df  |>
  mutate(clock_scorer = ifelse(dclkdraw == -3 | dclkdraw == -4 | dclkdraw == -7, 0,
                               #IMPUTE MEAN SCORE TO PERSONS MISSING A CLOCK*
                               #IF PROXY SAID CAN ASK SP*
                               ifelse(dclkdraw == -9 & speaktosp == 1, 2, 
                                      #IF SELF-RESPONDENT*       
                                      ifelse(dclkdraw == -9 & speaktosp == -1, 3, 
                                             ifelse(dclkdraw == -2 | dclkdraw == -9, NA, dclkdraw)))))


#MEMORY DOMAIN: IMMEDIATE AND DELAYED WORD RECALL 
df <- df |>
  mutate(irecall  =  ifelse(dwrdimmrc == -2 | dwrdimmrc == -1, NA,
                            ifelse(dwrdimmrc == -7 | dwrdimmrc == -3, 0, dwrdimmrc))) |> 
  mutate(irecall = ifelse(rnd==5 & dwrddlyrc==-9, NA, irecall)) |>  #round 5 only: set cases with missing word list and not previously assigned to missing
  
  mutate(drecall  =  ifelse(dwrddlyrc == -2 | dwrddlyrc == -1, NA,
                            ifelse(dwrddlyrc == -7 | dwrddlyrc == -3, 0, dwrddlyrc))) |> 
  mutate(drecall = ifelse(rnd==5 & dwrddlyrc==-9, NA, drecall)) |>  #round 5 only: set cases with missing word list and not previously assigned to missing
  
  mutate(wordrecall0_20 = irecall+drecall)


#CREATE COGNITIVE DOMAINS FOR ALL ELIGIBLE 

df<-df |> 
  mutate(clock65 = ifelse(clock_scorer == 0 | clock_scorer==1, 1, 
                          ifelse(clock_scorer > 1 & clock_scorer<6, 0, NA)))

df<-df |>  
  mutate(word65 = ifelse(wordrecall0_20 >= 0 & wordrecall0_20 <=3, 1, 
                         ifelse(wordrecall0_20 > 3 & wordrecall0_20 <=20, 0, NA)))

df<-df |>  
  mutate(datena65 = ifelse(date_prvp >= 0 & date_prvp <=3, 1, 
                           ifelse(date_prvp > 3 & date_prvp <= 8, 0, NA)))

#  *CREATE COGNITIVE DOMAIN SCORE*
df<-df |> 
  mutate(domain65 = clock65+word65+datena65)

#*SET CASES WITH MISSING WORD LIST AND NOT PREVIOUSLY ASSIGNED TO MISSING (ROUND 5 ONLY)
df<-df |>   
  mutate(demclas = ifelse(rnd==5 & dwrdlstnm==-9 & is.na(demclas), -9, demclas))

#UPDATE COGNITIVE CLASSIFICATION*
df<-df |> 
  #PROBABLE DEMENTIA
  mutate(demclas = ifelse(is.na(demclas) & (speaktosp == 1 | speaktosp == -1) & (domain65==2 | domain65==3), 1,
                          #POSSIBLE DEMENTIA
                          ifelse(is.na(demclas) & (speaktosp == 1 | speaktosp == -1) & domain65==1, 2,
                                 #NO DEMENITA                    
                                 ifelse(is.na(demclas) & (speaktosp == 1 | speaktosp == -1) & domain65==0, 3, demclas))))

#KEEP VARIABLES AND SAVE DATA
df<-df |> 
  dplyr::select(spid, rnd, demclas)

#CHANGE # AFTER "r" TO THE ROUND OF INTEREST
r11demclas <- df

#4. NAME AND SAVE DEMENTIA DATA FILE:
#CHANGE # AFTER "r" TO THE ROUND OF INTEREST
save(r11demclas, file = "~/R  HC/BMIN503_Final_Project/final final/NHATS_r11.dta") 

```

Once dementia class (demclas) is identified it is saved in the dataset 'df1'.

### Merging Datasets

```{r}
#merged datasets (md). md1 <- left_join(df, df1, by = "spid") md2 <- left_join(md1, df3, by = "spid")
#merged datasets (md). 
md1 <- left_join(df, df1, by = "spid")
md2 <- left_join(md1, df3, by = "spid")

# choose probable dementia and dementia patients who live at home
dementia1 <- md2 |>
  filter(demclas %in% c("1", "2") & (r11dresid  %in% c("1")))

dementia2 <- md2 |>
  filter(demclas %in% c("1", "2") & (r11dresid  %in% c("1", "2")))
```

## Preliminary Table Manipulation

After reviewing the literature on dementia caregivers and social determinants of health, as well as my clinical experiences, I selected the following variables for predictors and caregiving features. This process was also consulted with Dr. Demiris.

For data cleaning, I recoded the variables, most of which are categorical with many levels.

### Sociodemographic factors

Sociodemographic factors are identified as caregivers' age, race, gender, highest education level, and marital status, based on the previous literature(Rodakowski et al., 2012; Aman et al., 2020). These factors were also recoded accordingly. For instance, the education level of the caregivers was categorized as 'Less than high school (0)', 'High School (1)', and 'College or above (2)'. For economic status, the caregivers' reported income from the previous year was used."

```{r}
#Race
# Recode `race` to create a new binary variable
# 1 for "White, non-Hispanic" and 0 for "Non-White"
# Recode race to create a new variable 'race_recode'
dementia1 <- dementia1 |>
  mutate(
    race_recode = case_when(
      crl11dcgracehisp == 1 ~ 0,  # White, non-Hispanic
      crl11dcgracehisp == 2 ~ 1, #black, non-hispanic
      crl11dcgracehisp == 3 ~ 2,  # others
      crl11dcgracehisp == 4 ~ 3,
      chd11educ %in% c(5, 6) ~ NA_real_, # Missing or not applicable
      TRUE ~ NA_real_                      # Unhandled cases# hispanics
    ))  

table(dementia1$crl11dcgracehisp)
table(dementia1$race_recode)

# Gender: Male as reference (0), Female as 1
dementia1 <- dementia1 |>
  mutate(
    gender_recode = case_when(
      as.character(c11gender) == "1" ~ 0,  # Male
      as.character(c11gender) == "2" ~ 1,  # Female
      TRUE ~ NA_real_  # Handle any other unexpected cases
    )
  )

# Education: Recoding education levels into two categories
table(dementia1$chd11educ)
dementia1 <- dementia1 |>
  mutate(
    edu_recode = case_when(
      chd11educ %in% c(1, 2, 3) ~ 1,      # Below and high school diploma
      chd11educ %in% c(4, 5) ~ 1,          # Some college
      chd11educ %in% c(6, 7, 8) ~ 2,       # College and beyond
      chd11educ == c(9) ~ 3,  
      chd11educ %in% c(-8, -7, -6) ~ NA_real_, # Missing or not applicable
      TRUE ~ NA_real_                      # Unhandled cases
    )
  )
table(dementia1$edu_recode)

# Marital Status: Recoding marital status into binary (married vs. not married)
dementia1 <- dementia1 |>
  mutate(
    martstat_recode = case_when(
      chd11martstat == 1 ~ 0,     # Married
      chd11martstat %in% 2:6 ~ 1, # Not married (single, divorced, etc.)
      chd11martstat %in% c(-8, -6) ~ NA_real_, # Missing or not applicable
      TRUE ~ NA_real_                      # Unhandled cas
    )
  )

table(dementia1$martstat_recode)
```

```{r}
# preparing dataset separately for Summary Table 
final_dementia <- dementia1 |> 
mutate(
    race_recode = case_when(
      race_recode == 1 ~ "White, non-Hispanic",
      race_recode == 2 ~ "Black, non-Hispanic",
      race_recode == 3 ~ "Other",
      race_recode == 4 ~ "Hispanic",
      TRUE ~ NA_character_
    ),
    gender_recode = case_when(
      gender_recode == "0" ~ "Male",
      gender_recode == "1" ~ "Female",  
      TRUE ~ NA_character_
    ), 
    edu_recode = case_when(
      edu_recode == "1" ~ "Below high school",
      edu_recode == "2" ~ "Some college",
      edu_recode == "3" ~ "College and beyond",
      TRUE ~ NA_character_
    ),
    martstat_recode = case_when(
       martstat_recode == "2" ~ "Married",
      martstat_recode == "1" ~ "Not married",
      TRUE ~ NA_character_
    )
  )

# Dementia class -Adding labels for clarity
final_dementia$demclas_label <- factor(
final_dementia$demclas, 
  levels = c(1, 2), 
  labels = c("Probable Dementia", "Possible Dementia")
)

final_dementia <- final_dementia |>
  group_by(spid) |>
  slice_head(n = 1) |>
  ungroup()

#creating table, classfied by group 1 (proable dementia) and group 2 (possible dementia)
summary_table <- final_dementia |>
  select(demclas, race_recode, gender_recode, edu_recode, martstat_recode, chd11dage) |>
  tbl_summary(
    by = demclas,  
    statistic = list(all_continuous() ~ "{mean} ({sd})", all_categorical() ~ "{n} ({p}%)"),
    label = list(
      race_recode ~ "Race",
      gender_recode ~ "Gender",
      edu_recode ~ "Education",
      martstat_recode ~ "Marital Status",
      chd11dage ~ "Age"
    ),
    missing_text = "(Missing)"
  ) |>
  add_p() |>  # adding p-value 
  modify_header(label ~ "**Variable**") |>  
  bold_labels()  

# summarize 
summary_table
```

### Caregiving Features 

This selection is grounded in evidence from existing research and practical insights, aiming to provide a comprehensive understanding of the elements that most significantly impact caregiver well-being. The identified caregiving activities encompass a range of tasks and responsibilities that caregivers undertake, which can vary widely depending on the needs of the care recipient. These activities include providing physical care, managing medications, coordinating medical appointments, and offering emotional support. Additionally, the burden of caregiving can be influenced by the caregiver's own health, social support systems, and financial resources.

By focusing on these activities, the goal is to identify specific stressors and challenges that caregivers face, which can contribute to their mental health outcomes. Understanding these factors can help in developing targeted interventions and support systems to alleviate caregiver stress and improve their overall well-being. This approach not only highlights the importance of caregiving activities but also underscores the need for comprehensive support mechanisms to enhance the quality of life for both caregivers and care recipients.

According to NSOC codebooks, the variables starting with 'CCAs' represent caregivers' activities such as helping with ordering medications, online shopping, online billing, and caring for teeth. The variables starting with 'CACs' indicate caregivers' financial, emotional, and physical difficulties in helping persons with dementia (PWD). The variables starting with 'PA11' represent the social strains caregivers encounter during caregiving.

```{r}
#recoding some features
dementia1 <- dementia1 |>
  mutate(across(
    c(cca11hwoftchs, cca11hwoftshp, cca11hwoftpc, 
      cca11hwofthom, cca11hwoftdrv, cca11hwoftott), 
  )) |>
  mutate(
    cca11hwoftchs_recoded = case_when(
      cca11hwoftchs %in% c(1, 2, 3) ~ 1,
      cca11hwoftchs %in% c(4, 5) ~ 0,
      TRUE ~ NA_real_
    ),
    cca11hwoftshp_recoded = case_when(
      cca11hwoftshp %in% c(1, 2, 3) ~ 1,
      cca11hwoftshp %in% c(4, 5) ~ 0,
      TRUE ~ NA_real_
    ),
    cca11hwoftpc_recoded = case_when(
      cca11hwoftpc %in% c(1, 2, 3) ~ 1,
      cca11hwoftpc %in% c(4, 5) ~ 0,
      TRUE ~ NA_real_
    ),
    cca11hwofthom_recoded = case_when(
      cca11hwofthom %in% c(1, 2, 3) ~ 1,
      cca11hwofthom %in% c(4, 5) ~ 0,
      TRUE ~ NA_real_
    ),
    cca11hwoftdrv_recoded = case_when(
      cca11hwoftdrv %in% c(1, 2, 3) ~ 1,
      cca11hwoftdrv %in% c(4, 5) ~ 0,
      TRUE ~ NA_real_
    ),
    cca11hwoftott_recoded = case_when(
      cca11hwoftott %in% c(1, 2, 3) ~ 1,
      cca11hwoftott %in% c(4, 5) ~ 0,
      TRUE ~ NA_real_
    )
  )

dementia1 <- dementia1 |>
  mutate(across(
    c(cca11hlpordmd, cca11hlpbnkng, cca11cmpgrcry, 
      cca11cmpordrx, cca11cmpbnkng, cca11hlpteeth,cdc11hlpyrpls),
    ~ case_when(
        . == 1 ~ 1,
        . == 2 ~ 0,
        . %in% c(-8, -6, -1) ~ NA_real_,
        TRUE ~ NA_real_
      ),
    .names = "{.col}_recoded"
  ))
```

### **Outcomes (Anxiety and Depression)**

Caregivers' anxiety and depression are measured by two questions each. First, anxiety was measured Generalized Anxiety Disorder-2 (GAD-2) Scale which consists of two questions. Since the NHATS provided GAD-2 data, this study utilized it to measure anxiety levels among care recipients. Each item on the scale is rated on a four-point Likert scale, ranging from 0 (not at all) to 3 (nearly every day), resulting in a total score between 0 and 6. Higher scores correspond to greater anxiety, with a total GAD-2 score of 3 or more indicating anxiety. The care recipients' depression was evaluated using the Patient Health Questionnaire-2 (PHQ-2) Scale. Given that the NHATS included PHQ-2, this study utilized it to measure depression in care recipients. Each item on the scale was measured with a four-point Likert scale, ranging from 0 (not at all) to 3 (nearly every day), resulting a total score between 0 and 6, with higher scores indicating more severe depression. A PHQ-2 score ranges from 0-6. The authors identified a score of 3 as the optimal cutpoint when using the PHQ-2 to screen for depression. If the score is 3 or greater, major depressive disorder is likely.

```{r}
# Sum the two questions for GAD2
dementia1$total_gad2 <- dementia1$che11fltnervs + dementia1$che11fltworry
# Recode the combined variable using a cut-off of 3
dementia1$gad2_cg_cat <- ifelse(dementia1$total_gad2 < 3, 0, 1)
 table(dementia1$gad2_cg_cat)
 summary(dementia1$gad2_cg_cat) #1 ~ anxiety
# Sum of the two questions for PHQ2 (che11fltltlin + che11fltdown) 
dementia1$total_phq2 <- dementia1$che11fltltlin+ dementia1$che11fltdown
#Recode the combined variable using a cut-off of 3
dementia1$phq2_cg_cat <- ifelse(dementia1$total_phq2 < 3, 0, 1)
 table(dementia1$phq2_cg_cat)
 summary(dementia1$phq2_cg_cat) #1 ~ depression
```

## **Data analysis**

For data analysis, we first conducted descriptive analyses, including means, standard deviations, ranges, and percentages, to summarize the dataset. To investigate how caregivers' social strains and caregiver-level factors influence caregiver depression, we performed logistic regression analyses. Guided by the conceptual framework of this study, univariate logistic regression analyses were employed to identify caregivers' social strains and caregiver-level factors significantly associated with caregiver anxiety and depression, controlling for care recipient-level factors. Variables with a p-value below 0.05 in the univariate analyses were included in the subsequent multivariate logistic regression model. The multivariate model was then constructed to determine which factors most strongly influenced caregiver anxiety and depression. All statistical analyses were conducted using R, with statistical significance set at a p-value of less than 0.05.

### Pearson correlation matrix

```{r}
#choosing only one caregiver for each participant
final <- dementia1 |>
  group_by(spid) |>
  slice_head(n = 1) |>
  ungroup()

#total 563 caregivers- pwd dyads 
#creating subset to work more effectively

selected_vars <- c(
  "spid","demclas", "cca11hwoftchs", "cca11hwoftshp", "cca11hwoftpc",
  "cca11hwofthom", "cca11hwoftdrv", "cca11hwoftott",
  "che11enrgylmt", "cac11diffphy", "cac11exhaustd",
  "cac11toomuch", "cac11uroutchg", "cac11notime",
  "cac11diffemlv", "cpp11hlpkptgo", "che11health", 
  "che11sleepint", 
   "ew11progneed1", "ew11finhlpfam", 
  "mc11havregdoc", "hc11hosptstay", "hc11hosovrnht", 
  "cac11diffinc", "pa11hlkepfvst", "pa11hlkpfrclb", "pa11hlkpgoenj", "pa11hlkpfrwrk", "pa11hlkpfrvol", "pa11prcranoth", "race_recode", "gender_recode", "edu_recode", "chd11dage", "martstat_recode", "phq2_cg_cat", "gad2_cg_cat"
)

dementia_subset <- dementia1 |> select(all_of(selected_vars))
head(dementia_subset)

# Select only numeric columns
numeric_dementia1 <- dementia_subset |> select(where(is.numeric))

# Compute the Pearson correlation matrix and p-values
cor_matrix <- rcorr(as.matrix(numeric_dementia1), type = "pearson")

# Extract correlation coefficients and p-values
cor_coefficients <- cor_matrix$r
p_values <- cor_matrix$P

# Print the correlation coefficients and p-values
print("Pearson Correlation Coefficients:")
print("P-Values:")
```

Pearson's correlation matrices were presented to visually assess the data and evaluate the independence of variables. The original datasets contain approximately 1,700s variables, not all of which are included in these matrices due to their extensive length.

Despite the lengthy list of variables, I aimed to run these metrics to explore which features and sociodemographic factors are related to PHQ-2 (depression) and GAD-2 (anxiety). This analysis is crucial for identifying key predictors and understanding the complex relationships that influence mental health outcomes in caregivers.

### Imputation

The original dataset includes numerous negative and N/A values, and the sample size is small, necessitating preprocessing before feature selection. To handle the small sample size, I converted negative values in the `dementia_subset` dataframe to `NA` values. For continuous variables, I replaced `NA` values with the median of each variable. For categorical variables, I identified the most frequent level and replaced `NA` values with this level. This approach ensures that my dataset is clean and ready for analysis.

```{r}
#coverting negative value to NA
dementia_subset[dementia_subset < 0] <- NA

#imputation
continuous_vars <- sapply(dementia_subset, is.numeric)  # categorical
dementia_subset[continuous_vars] <- lapply(dementia_subset[continuous_vars], function(x) ifelse(is.na(x), median(x, na.rm = TRUE), x))

# categorical: change NA to max
categorical_vars <- sapply(dementia_subset, is.factor)  # find
dementia_subset[categorical_vars] <- lapply(dementia_subset[categorical_vars], function(x) {
  levels(x) <- append(levels(x), names(sort(table(x), decreasing = TRUE))[1])  
  ifelse(is.na(x), names(sort(table(x), decreasing = TRUE))[1], x)  
})

```

Based on my understanding of the Pearson correlation matrix, I selected these 15 variables to explore in this project. By carefully choosing these variables, I hoped to find significant correlations that could provide valuable insights into the factors influencing the outcomes of interest in my study. This selection process involved considering variables that are likely to have meaningful interactions and potential impacts on the research objectives. By focusing on these specific variables, I can ensure a more targeted and comprehensive analysis, which will enhance the overall quality and relevance of my research findings. This approach will help me draw more accurate and insightful conclusions, ultimately contributing to a deeper understanding of the subject matter.

```{r}
#final cleaning for dataset
final_dementia <- dementia_subset |>
  group_by(spid) |>
  slice_head(n = 1) |>
  ungroup()

#creating new dataset for each outcome
anxiety <- subset(
  final_dementia,
  select = c(gad2_cg_cat, cca11hwoftchs, cca11hwoftshp, cca11hwoftott, che11enrgylmt, 
             cac11diffphy, cac11exhaustd, cac11diffemlv, che11health, che11sleepint, ew11progneed1, race_recode, gender_recode, edu_recode, 
             chd11dage, martstat_recode)
)

depression <- subset(
  final_dementia,
  select = c(phq2_cg_cat, cca11hwoftchs, cca11hwoftshp, cca11hwoftott, che11enrgylmt, 
             cac11diffphy, cac11exhaustd, cac11diffemlv, che11health, che11sleepint, ew11progneed1, race_recode, gender_recode, edu_recode, 
             chd11dage, martstat_recode)
)
```

# Results

## Characteristics of Participants

I prepared the **final_dementia** dataset by recoding several variables to ensure consistency and clarity in the data. Next, I added more descriptive labels for dementia classes to improve interpretability. I also addressed missing and problematic values by converting negative values to **NA** and performing imputation. For continuous variables, I used the median to replace missing values, while for categorical variables, I substituted missing values with the most frequent category.

Finally, I generated a summary table that classifies the data by dementia status (Probable Dementia, N = 331 and Possible Dementia, N = 232) and included statistical tests to assess differences between the two groups. The comparison of variables across these groups revealed no significant differences for most of the categories examined. This suggests that, while there may be some variation in the data, the groups do not significantly differ in terms of the demographic and health-related factors assessed in the analysis.

```{r}
# preparing dataset - recoding some covariates
final_dementia <- final_dementia |> 
  mutate(
    race_recode = case_when(
      race_recode == 0 ~ "White, non-Hispanic",
      race_recode == 1 ~ "Black, non-Hispanic",
      race_recode == 2 ~ "Other",
      race_recode == 3 ~ "Hispanic",
      TRUE ~ NA_character_
    ),
    gender_recode = case_when(
      as.character(gender_recode) == "0" ~ "Male",
      as.character(gender_recode) == "1" ~ "Female",
      TRUE ~ NA_character_
    ),
    edu_recode = case_when(
      edu_recode == "1" ~ "Below high school",
      edu_recode == "2" ~ "Some college",
      edu_recode == "3" ~ "College and beyond",
      TRUE ~ NA_character_
    ),
    martstat_recode = case_when(
      martstat_recode == "1" ~ "Married",
      martstat_recode == "0" ~ "Not married",
      TRUE ~ NA_character_
    )
  )

# Dementia class -Adding labels for clarity
final_dementia$demclas_label <- factor(
final_dementia$demclas, 
  levels = c(1, 2), 
  labels = c("Probable Dementia", "Possible Dementia")
)
#coverting negative value to NA
final_dementia[final_dementia < 0] <- NA

#imputation
continuous_vars <- sapply(final_dementia, is.numeric)  # categorical
final_dementia[continuous_vars] <- lapply(final_dementia[continuous_vars], function(x) ifelse(is.na(x), median(x, na.rm = TRUE), x))

# categorical: change NA to max
categorical_vars <- sapply(final_dementia, is.factor)  # find
final_dementia[categorical_vars] <- lapply(final_dementia[categorical_vars], function(x) {
  levels(x) <- append(levels(x), names(sort(table(x), decreasing = TRUE))[1])  
  ifelse(is.na(x), names(sort(table(x), decreasing = TRUE))[1], x)  
})
#creating table, classfied by group 1 (proable dementia) and group 2 (possible dementia)
summary_table <- final_dementia |>
  select(demclas, race_recode, gender_recode, edu_recode, martstat_recode, chd11dage) |>
  tbl_summary(
    by = demclas,  
    statistic = list(all_continuous() ~ "{mean} ({sd})", all_categorical() ~ "{n} ({p}%)"),
    label = list(
      race_recode ~ "Race",
      gender_recode ~ "Gender",
      edu_recode ~ "Education",
      martstat_recode ~ "Marital Status",
      chd11dage ~ "Age"
    ),
    missing_text = "(Missing)"
  ) |>
  add_p() |>  # adding p-value 
  modify_header(label ~ "**Variable**") |>  
  bold_labels()  

# summarize 
summary_table

```

The table shows the distribution of variables by group, with p-values indicating statistical significance. For **Race**, the distribution differed significantly between the two groups (p = 0.005), with more Black, non-Hispanic individuals in group 2 (possible dementia group). **Gender** did not show a significant difference (p = 0.085), but there was a notable difference in **Education** (p = 0.005) with more individuals having a some college education in group 2 (Possible Dementia group). Both **Marital Status** (p = 0.003) and **Age** (p = 0.2) showed differences, but only **Marital Status** was statistically significant.

## Logistic regression

I conducted logistic regression first to understand the relationships between variables and establish a baseline model for comparison with more complex machine learning models. This approach helps identify significant predictors and provides a benchmark for performance.

```{r}
# Logistic regression for outcome gad2_cg_cat
model_gad2 <- glm(
  gad2_cg_cat ~ cca11hwoftchs + cca11hwoftshp + cca11hwoftott + che11enrgylmt + 
    cac11diffphy + cac11exhaustd + cac11diffemlv + che11health + che11sleepint + 
    ew11progneed1 + race_recode + gender_recode + edu_recode + chd11dage + 
    martstat_recode, 
  data = anxiety, 
  family = binomial(link = "logit")
)
summary(model_gad2)
# Logistic regression for outcome phq2_cg_cat
model_phq2 <- glm(
  phq2_cg_cat ~ cca11hwoftchs + cca11hwoftshp + cca11hwoftott + che11enrgylmt + 
    cac11diffphy + cac11exhaustd + cac11diffemlv + che11health + che11sleepint + 
    ew11progneed1 + race_recode + gender_recode + edu_recode + chd11dage + 
    martstat_recode, 
  data = depression, 
  family = binomial(link = "logit")
)
summary(model_phq2)
```

The logistic regression model identifies physical difficulty (cac11diffphy), exhaustion (cac11exhaustd), health status (che11health), sleep interruptions (che11sleepint), race (race_recode), gender (gender_recode), education level (edu_recode), and marital status (martstat_recode) are significantly associated with anxiety. The strongest predictor was caregiver's Exhaustion (cac11exhaustd) and race (race_recode) show the strongest associations with anxiety, with p-values \< 0.001.

The logistic regression model identifies several significant predictors of depression (PHQ-2) among caregivers. Key findings include: Physical difficulty (cac11diffphy), exhaustion (cac11exhaustd), health status (che11health), race (race_recode), gender (gender_recode), education level (edu_recode), and marital status (martstat_recode) are significantly negative associated with depression. Thestrongestpredictors was Exhaustion (cac11exhaustd) and race (race_recode) show the strongest associations with depression, with p-values \< 0.001.

```{r}
# Calculate odds ratios and confidence intervals for gad2_cg_cat model
odds_ratios_gad2 <- exp(coef(model_gad2))
conf_intervals_gad2 <- exp(confint(model_gad2))

# Combine into a tidy table for gad2_cg_cat
results_gad2 <- data.frame(
  Variable = names(odds_ratios_gad2),
  Odds_Ratio = odds_ratios_gad2,
  CI_Lower = conf_intervals_gad2[, 1],
  CI_Upper = conf_intervals_gad2[, 2]
)
print("Odds Ratios for gad2_cg_cat Model:")
print(results_gad2)

# Calculate odds ratios and confidence intervals for phq2_cg_cat model
odds_ratios_phq2 <- exp(coef(model_phq2))
conf_intervals_phq2 <- exp(confint(model_phq2))

# Combine into a tidy table for phq2_cg_cat
results_phq2 <- data.frame(
  Variable = names(odds_ratios_phq2),
  Odds_Ratio = odds_ratios_phq2,
  CI_Lower = conf_intervals_phq2[, 1],
  CI_Upper = conf_intervals_phq2[, 2]
)
print("Odds Ratios for phq2_cg_cat Model:")
print(results_phq2)

tidy_gad2 <- tidy(model_gad2, exponentiate = TRUE, conf.int = TRUE)
tidy_phq2 <- tidy(model_phq2, exponentiate = TRUE, conf.int = TRUE)

print("Tidy Results for gad2_cg_cat Model:")
print(tidy_gad2)

print("Tidy Results for phq2_cg_cat Model:")
print(tidy_phq2)
```

I calculated the odds ratios and confidence intervals for the `gad2_cg_cat` and `phq2_cg_cat` models. Both interpretations highlight the significant predictors and their associations with anxiety among caregivers.

## Machine Learning

This exploratory study employs two machine learning techniques—including Generalized Linear Model (GLM), and Random Forest (RF)—to identify key predictors of caregiver depression and anxiety. This exploratory approach is essential given the diverse types of data in this study. Machine learning methods, being inductive, support hypothesis generation and allow for systematic feature reduction by excluding variables deemed unimportant across multiple methods. This approach refines the feature set, enhancing the interpretability and predictive accuracy of the models.

### GLM - Anxiety

```{r}
#creating training/ testing data
anxiety_split <- initial_split(anxiety, 
                            prop = 0.80)
anxiety_split #<450/113/563>
anxiety_train <- training(anxiety_split)
anxiety_test <- testing(anxiety_split)

#glm
set.seed(123)
lr_class_spec<-logistic_reg()|>
  set_engine("glm")

anxiety_train$gad2_cg_cat <- as.factor(anxiety_train$gad2_cg_cat)
anxiety_test$gad2_cg_cat <- as.factor(anxiety_test$gad2_cg_cat)

lr_class_fit <- lr_class_spec |>
  fit(gad2_cg_cat ~ ., data = anxiety_train)

## top variables
significant_predictors <- tidy(lr_class_fit$fit) |>
  filter(p.value < 0.05)
print(significant_predictors)

### 
anxiety_glm_wf <- workflow() |>
  add_model(lr_class_spec) |>
  add_formula(gad2_cg_cat ~ .)

# Fit the workflow to the test data
anxiety_glm_fit <- anxiety_glm_wf |> 
  fit(data = anxiety_test)

# Generate predictions with probabilities
anxiety_glm_predicted <- predict(anxiety_glm_fit, new_data = anxiety_test, type = "prob")

anxiety_glm_predicted 

# Combine into a single data frame
anxiety_glm_pred_values <- bind_cols(
  truth = anxiety_test$gad2_cg_cat,  # Actual values of the outcome variable
  predict(anxiety_glm_fit, new_data = anxiety_test),  # Predicted class labels
  predict(anxiety_glm_fit, new_data = anxiety_test, type = "prob")  # Predicted probabilities
)

print(anxiety_glm_pred_values)
```

```{r}
#cross validation  20
anxiety_folds<-vfold_cv(anxiety_train, v=20)

glm_workflow<-workflow()|>
  add_model(lr_class_spec)|>
  add_formula(gad2_cg_cat ~ .)

glm_fit_cv<-glm_workflow|>
  fit_resamples(anxiety_folds, control=control_resamples(save_pred=TRUE))
collect_metrics(glm_fit_cv)


anxiety_glm_cv_preds<-collect_predictions(glm_fit_cv)
anxiety_glm_cv_preds |>
  group_by(id) |>
  roc_curve(truth = gad2_cg_cat, .pred_0) |>
  autoplot()

#Prediction on the test data
anxiety.lr.pred.values.test <-  bind_cols(
  truth = anxiety_test$gad2_cg_cat,
  predict(lr_class_fit, anxiety_test),
  predict(lr_class_fit, anxiety_test, type = "prob")
)
anxiety.lr.pred.values.test

#Plot of ROC curve of prediction on test results
autoplot(roc_curve(anxiety.lr.pred.values.test, 
                   truth, 
                   .pred_0))

#Metrics of prediction on test data
metrics(anxiety.lr.pred.values.test, truth, .pred_class, .pred_0)

```

In this step, I performed GLM on the `anxiety_train` dataset using a workflow that included cross-validation with 20 folds. Then, I evaluated the model's performance using ROC curves and various metrics on both the cross-validated predictions and the test data: ROC AUC (roc_auc): 0.916 and accuracy 0.841. I initially used 10-fold CV, but I encountered errors regarding new factors. After consulting with Dr. Himes, I tried 20-fold CV. I learned that 20-fold CV could work better for small datasets like mine.

### GLM - Depression

```{r}
#creating training/ testing data
depression_split <- initial_split(depression, 
                            prop = 0.80)
depression_split #<450/113/563>
depression_train <- training(depression_split)
depression_test <- testing(depression_split)

#glm
set.seed(123)
lr_class_spec<-logistic_reg()|>
  set_engine("glm")

depression_train$phq2_cg_cat <- as.factor(depression_train$phq2_cg_cat)
depression_test$phq2_cg_cat <- as.factor(depression_test$phq2_cg_cat)

lr_class_fit <- lr_class_spec |>
  fit(phq2_cg_cat ~ ., data = depression_train)

## top variables
significant_predictors_depression <- tidy(lr_class_fit$fit) |>
  filter(p.value < 0.05)
print(significant_predictors_depression)

### 
depression_glm_wf <- workflow() |>
  add_model(lr_class_spec) |>
  add_formula(phq2_cg_cat ~ .)

# Fit the workflow to the test data
depression_glm_fit <- depression_glm_wf |> 
  fit(data = depression_test)

# Generate predictions with probabilities
depression_glm_predicted <- predict(depression_glm_fit, new_data = depression_test, type = "prob")

depression_glm_predicted 

# Combine into a single data frame
depression_glm_pred_values <- bind_cols(
  truth = depression_test$phq2_cg_cat,  # Actual values of the outcome variable
  predict(depression_glm_fit, new_data = depression_test),  # Predicted class labels
  predict(depression_glm_fit, new_data = depression_test, type = "prob")  # Predicted probabilities
)

print(depression_glm_pred_values)

```

```{r}
#cross validation  20-fold
depression_folds<-vfold_cv(depression_train, v=20)

depression_glm_wf <- workflow ()|>
  add_model(lr_class_spec)|>
  add_formula(phq2_cg_cat ~ .)

depression_glm_fit_cv<-depression_glm_wf |>
  fit_resamples(depression_folds, control=control_resamples(save_pred=TRUE))
collect_metrics(depression_glm_fit_cv)


depression_glm_cv_preds<-collect_predictions(depression_glm_fit_cv)
depression_glm_cv_preds |>
  group_by(id) |>
  roc_curve(truth = phq2_cg_cat, .pred_0) |>
  autoplot()

#Prediction on the test data
depression.lr.pred.values.test <-  bind_cols(
  truth = depression_test$phq2_cg_cat,
  predict(lr_class_fit, depression_test),
  predict(lr_class_fit, depression_test, type = "prob")
)
depression.lr.pred.values.test

#Plot of ROC curve of prediction on test results
autoplot(roc_curve(depression.lr.pred.values.test, 
                   truth, 
                   .pred_0))

#Metrics of prediction on test data
metrics(depression.lr.pred.values.test, truth, .pred_class, .pred_0)

```

I performed a 20-fold cross-validation on a logistic regression model to predict depression (PHQ-2). The workflow was created, and the model was fitted using the cross-validation folds, achieving an AUC of 0.829. Predictions were collected and ROC curves were plotted to visualize the model's performance. Next, I made predictions on the test data and plotted the ROC curve for these predictions. The model's AUC on the test data was 0.884, indicating good predictive performance. Metrics for the test data predictions were also collected for further analysis.

### Random Forest (RF)- Anxiety

I specified a random forest model with 1,000 trees and a minimum node size of 5, using the `randomForest` engine for classification and enabling variable importance. I trained the model on the `anxiety_train` dataset and visualized the top predictors based on Mean Decrease Gini.

For model validation, I performed 20-fold cross-validation, integrating the random forest model into a workflow. The model achieved a strong **ROC-AUC score of 0.920**, demonstrating excellent classification performance.

```{r}

rf_spec<-rand_forest(trees=1000, min_n=5)|>
  set_engine("randomForest", importance=TRUE)|>
  set_mode("classification")

rf_fit<-rf_spec|>
  fit(gad2_cg_cat ~ ., data=anxiety_train)

## top variables
rf_fit|>
  extract_fit_engine()|>
  vip()

rf_fit|>
  extract_fit_engine()|>
  importance()|>
  as.data.frame()|>
  arrange(desc(MeanDecreaseGini))

#20fold CV- rf
anxiety_folds<-vfold_cv(anxiety_train, v=20)
rf_workflow <- workflow() |>
  add_model(rf_spec) |>
  add_formula(gad2_cg_cat ~ .)

rf_fit_cv <- rf_workflow |>
  fit_resamples(anxiety_folds, control=control_resamples(save_pred=TRUE))
collect_metrics(rf_fit_cv) 
```

```{r}
anxiety_rf_cv_preds<-collect_predictions(rf_fit_cv)
anxiety_rf_cv_preds|>
  group_by(id)|>
  roc_auc(truth = gad2_cg_cat, .pred_0)

# Plot ROC curve
anxiety_rf_cv_preds |>
  group_by(id)|>
  roc_curve(truth = gad2_cg_cat, .pred_0) |>
  autoplot()

# Fit the random forest model on the full training data
anxiety_rf_fit <- rf_spec |>
  fit(gad2_cg_cat ~ ., data = anxiety_train)

anxiety_rf_fit

#testing
anxiety_rf_pred_values <- bind_cols(
  truth = anxiety_test$gad2_cg_cat,  # Actual values of the outcome variable
  predict(anxiety_rf_fit, new_data = anxiety_test),  # Predicted class labels
  predict(anxiety_rf_fit, new_data = anxiety_test, type = "prob")  # Predicted probabilities
)
roc_auc(anxiety_rf_pred_values,
        truth, 
        .pred_0)

autoplot(roc_curve(anxiety_rf_pred_values, 
                   truth, 
                   .pred_0))

```

I did cross-validation predictions and calculated the **ROC-AUC** for each fold. Then, I plotted the **ROC curve** for the cross-validation results. Afterward, I fitted the random forest model to the full training data (`anxiety_train`) and predicted class labels and probabilities on the test data, achieving a **ROC-AUC** of 0.926. Finally, I plotted the **ROC curve** for the test set predictions.

```{r}
anxiety_rf_fit |>
  extract_fit_engine() |>
  importance()

anxiety_rf_fit |>
  extract_fit_engine() |>
  vip()
```

I extracted the fitted random forest model for anxiety (`anxiety_rf_fit`) and calculated the importance of each predictor variable using the `importance()` function. This step helped identify which variables had the most significant impact on the model's predictions. To further understand these contributions, I visualized the variable importance using the `vip()` function.

The analysis revealed that the top five variables influencing the model were:

1.  **cac11exhaustd**: Caregivers' feeling of exhaustion.

2.  **cac11diffphy**: Caregivers' difficulty with providing physical help.

3.  chd11dge: Caregivers' age

4.  **che11health**: Caregivers' own health problems.

5.  **che11energylmt**: Caregivers' feeling of lacking energy.

These findings highlight the significant factors contributing to anxiety among caregivers, emphasizing the physical and emotional challenges they face.

```{r}
#20-fold cross validation on rf
anxiety_folds<-vfold_cv(anxiety_train, v=20)

rf_workflow <- workflow() |>
  add_model(rf_spec) |>
  add_formula(gad2_cg_cat ~ .)

rf_fit_cv_anxiety <- rf_workflow |>
  fit_resamples(resamples = anxiety_folds)

collect_metrics(rf_fit_cv_anxiety)

rf_wf_fit_cv_anxiety <- rf_workflow |>
  fit_resamples(
    resamples = anxiety_folds,
    control = control_resamples(save_pred = TRUE)
  )

collect_metrics(rf_wf_fit_cv_anxiety) 	

rf_wf_fit_cv_anxiety |>
  collect_predictions() |>
  roc_curve(gad2_cg_cat, .pred_0) |>
  autoplot()

# Collect metrics from the resampling results
rf_wf_fit_cv_anxiety_metrics <- collect_metrics(rf_wf_fit_cv_anxiety)

```

The random forest model demonstrated strong performance with consistent **ROC AUC** values across different evaluations: 0.908, and 0.911. These results indicate the model's ability to effectively distinguish between the classes. The ROC curve further confirmed this, showing the model's good classification ability.

### Random Forest - Depression

I created a random forest model with 1000 trees and a minimum node size of 5 to classify depression using the `phq2_cg_cat` variable.

```{r}

rf_spec<-rand_forest(trees=1000, min_n=5)|>
  set_engine("randomForest", importance=TRUE)|>
  set_mode("classification")

depression_rf_fit<-rf_spec|>
  fit(phq2_cg_cat ~ ., data=depression_train)

#20fold CV- rf
depression_folds<-vfold_cv(depression_train, v=20)
depression_rf_workflow <- workflow() |>
  add_model(rf_spec) |>
  add_formula(phq2_cg_cat ~ .)

depression_rf_fit_cv <- depression_rf_workflow |>
  fit_resamples(depression_folds, control=control_resamples(save_pred=TRUE))
collect_metrics(depression_rf_fit_cv) 
```

I performed 20-Fold cross-validation on the depression dataset using a random forest workflow and collected metrics to evaluate the model's performance, ROC-AUC of 0.885, accuracy of 0.817.

```{r}
depression_rf_cv_preds<-collect_predictions(depression_rf_fit_cv)

depression_rf_cv_preds|>
  group_by(id)|>
  roc_auc(truth = phq2_cg_cat, .pred_0)

# Plot ROC curve
depression_rf_cv_preds |>
  group_by(id)|>
  roc_curve(truth = phq2_cg_cat, .pred_0) |>
  autoplot()

# Fit the random forest model on the full training data
depression_rf_fit <- rf_spec |>
  fit(phq2_cg_cat ~ ., data = depression_train)

depression_rf_fit

#testing
depression_rf_pred_values <- bind_cols(
  truth = depression_test$phq2_cg_cat,  # Actual values of the outcome variable
  predict(depression_rf_fit, new_data = depression_test),  # Predicted class labels
  predict(depression_rf_fit, new_data = depression_test, type = "prob")  # Predicted probabilities
)
roc_auc(depression_rf_pred_values,
        truth, 
        .pred_0)

autoplot(roc_curve(depression_rf_pred_values, 
                   truth, 
                   .pred_0))

```

I collected predictions from the cross-validated random forest model for depression and calculated the ROC AUC for each fold. I then plotted the ROC curve to visualize the model's performance. After fitting the random forest model on the full training data, I tested it on the test data, calculated the ROC AUC, and plotted the ROC curve for the test predictions.

```{r}
depression_rf_fit |>
  extract_fit_engine() |>
  importance()

depression_rf_fit |>
  extract_fit_engine() |>
  vip()

```

The analysis revealed that the top five variables influencing depression are similar to those in the anxiety model, but they appear in a different order of importance. Notably, `race_recode` emerged as a more prominent factor in predicting depression compared to anxiety. This suggests that the race of the caregiver plays a significant role in depression, highlighting a key difference in the factors contributing to these two mental health conditions. This insight is crucial as it underscores the importance of considering racial and ethnic backgrounds when developing interventions and support programs for caregivers. Tailoring support based on race can help address specific challenges and needs, ultimately improving the mental health and well-being of caregivers from diverse backgrounds. This finding emphasizes the need for culturally sensitive approaches in mental health care and support services.

```{r}
rf_spec_depression<-rand_forest(trees=1000, min_n=5)|>
  set_engine("randomForest", importance=TRUE)|>
  set_mode("classification")

rf_fit_depression<-rf_spec_depression|>
  fit(phq2_cg_cat ~ ., data=depression_train)

#20-fold cross validation on rf
depression_folds<-vfold_cv(depression_train, v=20)

rf_workflow_depression <- workflow() |>
  add_model(rf_spec_depression) |>
  add_formula(phq2_cg_cat ~ .)

rf_fit_cv_depression <- rf_workflow_depression |>
  fit_resamples(resamples = depression_folds)

collect_metrics(rf_fit_cv_depression)

rf_wf_fit_cv_depression <- rf_workflow_depression |>
  fit_resamples(
    resamples = depression_folds,
    control = control_resamples(save_pred = TRUE)
  )

collect_metrics(rf_wf_fit_cv_depression) 	

rf_wf_fit_cv_depression |>
  collect_predictions() |>
  roc_curve(phq2_cg_cat, .pred_0) |>
  autoplot()

```

I created a random forest model specification (`rf_spec_depression`) with 1000 trees and a minimum node size of 5, setting it to use the "randomForest" engine with importance measures enabled. This model was then fitted to the depression training data (`phq2_cg_cat`). Next, I performed 20-fold cross-validation on this random forest model using the training data. The workflow was created and fitted to the cross-validation folds, achieving an initial AUC of 0.881. After saving predictions, the AUC was slightly adjusted to 0.8791. I also plotted the ROC curve to visualize the model's performance and collected metrics from the resampling results to evaluate the model's effectiveness.

## **Comparison of ROC Curves for Logistic Regression and Random Forest Models in Anxiety and Depression Prediction**

I generated ROC curves for both the logistic regression and random forest models on the training and cross-validation sets for anxiety and depression prediction. The ROC curves were plotted using `ggplot2` to compare the performance of the models. The plot includes: **Logistic Regression (Training)**: Solid blue line, **Logistic Regression (20-fold CV)**: Dashed green line, **Random Forest (Training)**: Solid red line, and **Random Forest (20-fold CV)**: Dashed purple line. This visualization helps in comparing the true positive rate (sensitivity) against the false positive rate (1 - specificity) for each model and dataset, providing insights into their predictive performance.

### Anxiety

```{r}
# ROC curve for GLM training set
anxiety_roc_glm_training <- anxiety_glm_pred_values |> roc_curve(truth, .pred_0)
# ROC curve for GLM cross-validation set
anxiety_roc_glm_cv <- anxiety.lr.pred.values.test |> roc_curve(truth, .pred_0)
# ROC curve for RF training set
anxiety_roc_rf_training <- anxiety_rf_pred_values|> roc_curve(truth, .pred_0)
# ROC curve for RF cross-validation set
anxiety_roc_rf_cv <- anxiety_rf_cv_preds |>
  roc_curve(truth = gad2_cg_cat, .pred_0)


ggplot() +
  geom_path(data = anxiety_roc_glm_training, aes(x = 1 - specificity, y = sensitivity), 
            color = "blue", linetype = "solid", size = 1, label = "Logistic Regression (Training)") +
  geom_path(data = anxiety_roc_glm_cv, aes(x = 1 - specificity, y = sensitivity), 
            color = "green", linetype = "dashed", size = 1, label = "Logistic Regression (20-fold CV)") +
  geom_path(data = anxiety_roc_rf_training, aes(x = 1 - specificity, y = sensitivity), 
            color = "red", linetype = "solid", size = 1, label = "Random Forest (Training)") +
  geom_path(data = anxiety_roc_rf_cv, aes(x = 1 - specificity, y = sensitivity), 
            color = "purple", linetype = "dashed", size = 1, label = "Random Forest (20-fold CV)") +
coord_equal() +
  theme_bw() +
  labs(title = "Anxiety ROC Curves for Logistic Regression and Random Forest",
       x = "1 - Specificity (False Positive Rate)",
       y = "Sensitivity (True Positive Rate)",
       color = "Model", 
       linetype = "Type") +
  theme(legend.position = "right") + 
   geom_text(aes(x = 0.7, y = 0.2, label = "Logistic Regression (Training)"), 
            color = "blue", size = 2) +
  geom_text(aes(x = 0.7, y = 0.15, label = "Logistic Regression (20-fold CV)"), 
            color = "green", size = 2, linetype = "dashed") +
  geom_text(aes(x = 0.7, y = 0.1, label = "Random Forest (Training)"), 
            color = "red", size = 2) +
  geom_text(aes(x = 0.7, y = 0.05, label = "Random Forest (20-fold CV)"), 
            color = "purple", size = 2, linetype = "dashed")
```

The plot shows that logistic regression (training) has the highest ROC-AUC, followed by random forest (training). The 20-fold cross-validated logistic regression has a slightly better AUC of 0.916, while the 20-fold cross-validated random forest has the lowest AUC of 0.884.

### Depression

```{r}

# ROC curve for GLM training set
depression_roc_glm_training <- depression_glm_pred_values |>
  roc_curve(truth = truth, .pred_0)

# ROC curve for GLM cross-validation set
depression_roc_glm_cv <- depression.lr.pred.values.test |>
  roc_curve(truth = truth, .pred_0)

# ROC curve for RF training set
depression_roc_rf_training <- depression_rf_pred_values |>
  roc_curve(truth = truth, .pred_0)

# ROC curve for RF cross-validation set
depression_roc_rf_cv <- depression_rf_cv_preds |>
  roc_curve(truth = phq2_cg_cat, .pred_0)


ggplot() +
  geom_path(data = depression_roc_glm_training, aes(x = 1 - specificity, y = sensitivity), 
            color = "blue", linetype = "solid", size = 1, label = "Logistic Regression (Training)") +
  geom_path(data = depression_roc_glm_cv, aes(x = 1 - specificity, y = sensitivity), 
            color = "green", linetype = "dashed", size = 1, label = "Logistic Regression (20-fold CV)") +
  geom_path(data = depression_roc_rf_training, aes(x = 1 - specificity, y = sensitivity), 
            color = "red", linetype = "solid", size = 1, label = "Random Forest (Training)") +
  geom_path(data = depression_roc_rf_cv, aes(x = 1 - specificity, y = sensitivity), 
            color = "purple", linetype = "dashed", size = 1, label = "Random Forest (20-fold CV)") +
coord_equal() +
  theme_bw() +
  labs(title = "Depression ROC Curves for Logistic Regression and Random Forest",
       x = "1 - Specificity (False Positive Rate)",
       y = "Sensitivity (True Positive Rate)",
       color = "Model", 
       linetype = "Type") +
  theme(legend.position = "right") + 
   geom_text(aes(x = 0.5, y = 0.2, label = "Logistic Regression (Training)"), 
            color = "blue", size = 2) +
  geom_text(aes(x = 0.5, y = 0.15, label = "Logistic Regression (20-fold CV)"), 
            color = "green", size = 2, linetype = "dashed") +
  geom_text(aes(x = 0.5, y = 0.1, label = "Random Forest (Training)"), 
            color = "red", size = 2) +
  geom_text(aes(x = 0.5, y = 0.05, label = "Random Forest (20-fold CV)"), 
            color = "purple", size = 2, linetype = "dashed")
```

The plot shows that logistic regression (training) has the highest ROC-AUC, followed by random forest (training). The 20-fold cross-validated logistic regression has a slightly better AUC of 0.884, while the 20-fold cross-validated random forest has the lowest AUC of 0.879.

Both the depression and anxiety datasets have shown that the 20-fold cross-validated logistic regression model has a slightly better AUC compared to the 20-fold cross-validated random forest model. This indicates that logistic regression performs marginally better in terms of distinguishing between classes for both datasets. The slightly better AUC for the 20-fold cross-validated logistic regression suggests it might be the preferred model for both depression and anxiety datasets. This is because it performs marginally better in distinguishing between classes.

The consistent performance across both datasets indicates that logistic regression is a robust choice for similar types of classification problems. This can guide future modeling efforts in subsequent studies that I plan to undertake.

Regarding my research question for this project, I identified several key predictors of anxiety and depression among caregivers of PWD. These predictors include exhaustion, lack of energy, difficulty with physical help, sleep issues, health problems, race, and age. These factors highlight the multifaceted challenges faced by caregivers. Additionally, the discovery that caregiver race and age are significant predictors of depression and anxiety opens new avenues for targeted interventions aimed at improving caregiver mental health.

This research aimed to identify these key predictors and evaluate the predictive accuracy of machine learning algorithms in supporting the early identification of at-risk caregivers. By leveraging advanced techniques, we can enhance early detection, enabling more timely interventions to mitigate mental health risks.

Given that this is an exploratory study, the findings serve as a foundation for future research. I hope to further refine the model by applying a more diverse set of feature selection techniques, such as LASSO. By doing so, I aim to identify the most relevant predictors of caregiver mental health outcomes, allowing for a better-performing and more interpretable model. Additionally, I plan to explore the interaction effects between selected features, as these interactions might uncover more nuanced relationships that single predictors alone cannot capture.

Since this is a longitudinal dataset, I plan to obtain another round of NHATS and NSOC data to identify trends and patterns, enabling a more comprehensive analysis and improving support programs for caregivers. Combining these advanced feature selection techniques with longitudinal data will also allow us to track the impact of specific factors over time and adjust the model accordingly.

**Limitations**:\
Despite the promising results, this study has several limitations. The sample size for both the training and testing sets may not fully capture the diversity of the target population, limiting the generalizability of the findings. While cross-validation reduces overfitting, it does not account for all potential biases or variations within different subgroups. Additionally, missing data, if not properly addressed, could introduce bias or reduce model accuracy. The models used are based on observed data and may not fully capture complex relationships that could emerge with a broader set of variables or external factors. Finally, the reliance on specific features may limit the model's performance in more dynamic real-world scenarios.

# Conclusion

This exploratory study provides valuable insights into the caregivers' factors influencing anxiety and depression among caregivers and lays the groundwork for future research aimed at improving caregiver mental health. By identifying the key predictors of mental health outcomes and evaluating the predictive power of machine learning algorithms, the study highlights the potential for early identification and intervention for at-risk caregivers. Further validation, particularly through longitudinal studies, is essential to deepen our understanding and inform the development of tailored interventions to support the well-being of caregivers in the long term.
